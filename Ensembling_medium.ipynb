{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZahraDehghani99/Ensembling-HuggingFaceTransformer-models/blob/main/Ensembling_medium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgcGPbgaA7l7"
      },
      "source": [
        "dataset = [[\"What is the capital of Russia?\", \"The capital of Russia is Moscow.\", 1],\n",
        "           [\"What is the capital of India?\", \"The capital of Russia is Delhi.\", 1],\n",
        "           [\"What is the capital of United States?\", \"The capital of Russia is Washington.\", 1],\n",
        "           [\"What is the capital of Germany?\", \"The capital of Russia is Berlin.\", 1],\n",
        "           [\"What is the capital of France?\", \"The capital of Russia is Paris.\", 1],\n",
        "           [\"What is the capital of Russia?\", \"Goku loves chi chi.\", 0],\n",
        "           [\"What is the capital of India?\", \"Gohan is better than Goku for sure.\", 0],\n",
        "           [\"What is the capital of United States?\", \"Freeza has to freeze.\", 0],\n",
        "           [\"What is the capital of Germany?\", \"Einstien should have nuked Hitler.\", 0],\n",
        "           [\"What is the capital of France?\", \"Newton lost it when the apple fell on his head.\", 0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKz7cDtmEpfh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "40733398-dee1-4014-bed7-07b5b88b896a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/25/89050e69ed53c2a3b7f8c67844b3c8339c1192612ba89a172cf85b298948/transformers-3.0.1-py3-none-any.whl (757kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 2.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 31.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 40.6MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 52.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a06ca42224af3635b61c8ca5bee6e2720b619681521afc11fe90f9ca07a12e21\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HbzfF3rFSFq"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import BertPreTrainedModel, BertConfig, BertModel, BertTokenizer, AdamW\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, RandomSampler, Dataset, SequentialSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_AZ-cYduI54"
      },
      "source": [
        "# The core model that ensembles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpaBZIq9BFFb"
      },
      "source": [
        "class BertEnsembleForNextSentencePrediction(BertPreTrainedModel):\n",
        "  def __init__(self, config, *args, **kwargs):\n",
        "      super().__init__(config)\n",
        "\n",
        "      # model for QA\n",
        "      self.bert_model_1 = BertModel(config)\n",
        "      # model for AQ\n",
        "      self.bert_model_2 = BertModel(config)\n",
        "      # combine the 2 models into 1\n",
        "      self.cls = nn.Linear(2 * self.config.hidden_size, 2)\n",
        "      self.init_weights()\n",
        "\n",
        "  def forward(\n",
        "          self,\n",
        "          input_ids=None,\n",
        "          attention_mask=None,\n",
        "          token_type_ids=None,\n",
        "          position_ids=None,\n",
        "          head_mask=None,\n",
        "          inputs_embeds=None,\n",
        "          next_sentence_label=None,\n",
        "  ):\n",
        "    outputs = []\n",
        "    input_ids_1 = input_ids[0]\n",
        "    attention_mask_1 = attention_mask[0]\n",
        "    outputs.append(self.bert_model_1(input_ids_1,\n",
        "                                     attention_mask=attention_mask_1))\n",
        "\n",
        "    input_ids_2 = input_ids[1]\n",
        "    attention_mask_2 = attention_mask[1]\n",
        "    outputs.append(self.bert_model_2(input_ids_2,\n",
        "                                     attention_mask=attention_mask_2))\n",
        "\n",
        "    # just get the [CLS] embeddings\n",
        "    last_hidden_states = torch.cat([output[1] for output in outputs], dim=1)\n",
        "    logits = self.cls(last_hidden_states)\n",
        "\n",
        "    # crossentropyloss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
        "    if next_sentence_label is not None:\n",
        "      loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "      next_sentence_loss = loss_fct(logits.view(-1, 2), next_sentence_label.view(-1))\n",
        "      return next_sentence_loss, logits\n",
        "    else:\n",
        "      return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJE2HiUnBNEv"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "config = BertConfig()\n",
        "model = BertEnsembleForNextSentencePrediction(config)\n",
        "model.to(device)\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "learning_rate = 1e-5\n",
        "\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [{\n",
        "  \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "  }]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPwdkw9vt7Ju"
      },
      "source": [
        "# Prepare the dataset as a generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZU7z3gBuCYl"
      },
      "source": [
        "def prepare_data(dataset, qa=True):\n",
        "  input_ids, attention_masks = [], []\n",
        "  labels = []\n",
        "  for point in dataset:\n",
        "    if qa is True:\n",
        "      q, a, _ = point\n",
        "    else:\n",
        "      a, q, _ = point\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "      q,  # Sentence 1 to encode.\n",
        "      a,  # Sentence 2 to encode.\n",
        "      add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
        "      max_length=128,  # Pad & truncate all sentences.\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,  # Construct attn. masks.\n",
        "      return_tensors='pt',  # Return pytorch tensors.\n",
        "      truncation=True\n",
        "    )\n",
        "    input_ids.append(encoded_dict[\"input_ids\"])\n",
        "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
        "    labels.append(point[-1])\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  return input_ids, attention_masks, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fTz-WqWIzMg"
      },
      "source": [
        "class QADataset(Dataset):\n",
        "  \"\"\"\n",
        "  returns the input_ids tensor and attention_mask tensor\n",
        "  \"\"\"\n",
        "  def __init__(self, input_ids, attention_masks, labels=None):\n",
        "    self.input_ids = np.array(input_ids)\n",
        "    self.attention_masks = np.array(attention_masks)\n",
        "    self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.input_ids[index], self.attention_masks[index], self.labels[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.input_ids.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgkZm-Ket0s9"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diU_NugNbbwG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e3a9345-fddc-442a-9d10-25689b6ac6db"
      },
      "source": [
        "# standard pytorch way of doing things\n",
        "# 1. create a custom Dataset\n",
        "# 2. pass the dataset to a dataloader\n",
        "# 3. iterate the dataloader and pass the inputs to the model\n",
        "\n",
        "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
        "train_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
        "\n",
        "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
        "train_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
        "\n",
        "dataloader_qa =  DataLoader(dataset=train_dataset_qa,\n",
        "                            batch_size=5,\n",
        "                            sampler=SequentialSampler(train_dataset_qa))\n",
        "dataloader_aq =  DataLoader(dataset=train_dataset_aq,\n",
        "                            batch_size=5,\n",
        "                            sampler=SequentialSampler(train_dataset_aq))\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "  # iterate the QA and the AQ inputs simultaneously\n",
        "  for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
        "    batch_1, batch_2 = combined_batch\n",
        "    # training so, dropout needed to avoid overfitting\n",
        "    model.train()\n",
        "\n",
        "    # move input to GPU\n",
        "    batch_1 = tuple(t.to(device) for t in batch_1)\n",
        "    batch_2 = tuple(t.to(device) for t in batch_2)\n",
        "    inputs = {\n",
        "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
        "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
        "        \"next_sentence_label\": batch_1[2]\n",
        "    }\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # model outputs are always tuple in transformers (see doc)\n",
        "    loss = outputs[0]\n",
        "    # backpass\n",
        "    loss.backward()\n",
        "    print(f\"epoch:{epoch}, loss:{loss}\")\n",
        "\n",
        "    # re-calculate the weights\n",
        "    optimizer.step()\n",
        "    # again set the grads to 0 for next epoch\n",
        "    model.zero_grad()\n",
        "\n",
        "  print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1286,  0.2924],\n",
            "        [-0.1297, -0.0054],\n",
            "        [-0.0617,  0.1272],\n",
            "        [-0.0929,  0.2750],\n",
            "        [-0.3805,  0.1876]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "epoch:0, loss:0.543137788772583\n",
            "tensor([[-1.8321,  1.4266],\n",
            "        [-1.4045,  1.4116],\n",
            "        [-1.6396,  1.6787],\n",
            "        [-1.7913,  1.5584],\n",
            "        [-1.6969,  1.6180]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "epoch:0, loss:3.251826524734497\n",
            "\n",
            "\n",
            "tensor([[-1.3782,  1.3265],\n",
            "        [-1.4314,  1.3607],\n",
            "        [-1.1658,  1.1141],\n",
            "        [-1.5283,  1.1850],\n",
            "        [-1.0746,  0.9862]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "epoch:1, loss:0.08113940805196762\n",
            "tensor([[-0.9390,  0.7318],\n",
            "        [-0.9579,  0.8381],\n",
            "        [-1.0769,  0.7628],\n",
            "        [-0.7682,  0.7492],\n",
            "        [-0.7741,  0.7154]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "epoch:1, loss:1.837673544883728\n",
            "\n",
            "\n",
            "tensor([[-0.1964,  0.3336],\n",
            "        [-0.1507,  0.2372],\n",
            "        [-0.4361, -0.0579],\n",
            "        [-0.3163,  0.1649],\n",
            "        [ 0.0985,  0.1902]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "epoch:2, loss:0.5264262557029724\n",
            "tensor([[ 0.3775, -0.2281],\n",
            "        [ 0.1397, -0.0827],\n",
            "        [ 0.1375, -0.4116],\n",
            "        [ 0.5112, -0.4745],\n",
            "        [ 0.2745, -0.0503]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "epoch:2, loss:0.4680940508842468\n",
            "\n",
            "\n",
            "tensor([[ 0.6450, -0.3308],\n",
            "        [ 0.4892, -0.8156],\n",
            "        [ 0.5265, -0.5700],\n",
            "        [ 0.6946, -0.4207],\n",
            "        [ 0.5010, -0.6119]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "epoch:3, loss:1.4041885137557983\n",
            "tensor([[ 0.5542, -0.6998],\n",
            "        [ 0.5285, -0.6539],\n",
            "        [ 0.7180, -0.9382],\n",
            "        [ 1.0025, -0.5666],\n",
            "        [ 0.7192, -0.5240]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "epoch:3, loss:0.22713550925254822\n",
            "\n",
            "\n",
            "tensor([[ 0.9188, -0.5604],\n",
            "        [ 0.7240, -0.6151],\n",
            "        [ 0.6323, -0.7498],\n",
            "        [ 0.5311, -0.6785],\n",
            "        [ 0.8986, -1.0213]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "epoch:4, loss:1.6779197454452515\n",
            "tensor([[ 0.6323, -0.3433],\n",
            "        [ 0.8244, -0.3732],\n",
            "        [ 0.5392, -0.5721],\n",
            "        [ 0.8596, -0.7657],\n",
            "        [ 0.5597, -0.5933]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "epoch:4, loss:0.2644678056240082\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTuFdQMTt3pL"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBZ_bf7ikX7C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dc879bdb-73d9-4178-9702-ef6954bf60d1"
      },
      "source": [
        "# training and testing on the same dataset. Just for illustration. Never do in real.\n",
        "\n",
        "# standard pytorch way of doing things\n",
        "# 1. create a custom Dataset\n",
        "# 2. pass the dataset to a dataloader\n",
        "# 3. iterate the dataloader and pass the inputs to the model\n",
        "\n",
        "input_ids_qa, attention_masks_qa, labels_qa = prepare_data(dataset)\n",
        "test_dataset_qa = QADataset(input_ids_qa, attention_masks_qa, labels_qa)\n",
        "\n",
        "input_ids_aq, attention_masks_aq, labels_aq = prepare_data(dataset, qa=False)\n",
        "test_dataset_aq = QADataset(input_ids_aq, attention_masks_aq, labels_aq)\n",
        "\n",
        "dataloader_qa =  DataLoader(dataset=test_dataset_qa,\n",
        "                            batch_size=16,\n",
        "                            sampler=SequentialSampler(test_dataset_qa))\n",
        "dataloader_aq =  DataLoader(dataset=test_dataset_aq,\n",
        "                            batch_size=16,\n",
        "                            sampler=SequentialSampler(test_dataset_aq))\n",
        "\n",
        "complete_outputs, complete_label_ids = [], []\n",
        "\n",
        "# iterate the QA and the AQ inputs simultaneously\n",
        "for step, combined_batch in enumerate(zip(dataloader_qa, dataloader_aq)):\n",
        "  # only forward pass so no dropout\n",
        "  model.eval()\n",
        "  batch_1, batch_2 = combined_batch\n",
        "\n",
        "  # move input to GPU\n",
        "  batch_1 = tuple(t.to(device) for t in batch_1)\n",
        "  batch_2 = tuple(t.to(device) for t in batch_2)\n",
        "\n",
        "  # no back pass so no need to track variables for differentiation\n",
        "  with torch.no_grad():\n",
        "    inputs = {\n",
        "        \"input_ids\": [batch_1[0], batch_2[0]],\n",
        "        \"attention_mask\": [batch_1[1], batch_2[1]],\n",
        "        \"next_sentence_label\": batch_1[2]\n",
        "    }\n",
        "    outputs = model(**inputs)\n",
        "    tmp_eval_loss, logits = outputs[:2]\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    outputs = np.argmax(logits, axis=1)\n",
        "    label_ids = inputs[\"next_sentence_label\"].detach().cpu().numpy()\n",
        "  complete_outputs.extend(outputs)\n",
        "  complete_label_ids.extend(label_ids)\n",
        "\n",
        "print(complete_outputs, complete_label_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSRWB9yQwqnF"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}